{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uYBg0EPzplSS"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBy0pq3EpZ9Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "!pip install tensorflow keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp2xTFKrpZ9a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import glob\n",
        "import os\n",
        "\n",
        "from keras import Input\n",
        "from keras.applications import VGG19\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.layers import BatchNormalization, Activation, LeakyReLU, Add, Dense\n",
        "from keras.layers import Conv2D, UpSampling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "import random\n",
        "from numpy import asarray\n",
        "from itertools import repeat\n",
        "\n",
        "import imageio\n",
        "from imageio import imread\n",
        "from PIL import Image\n",
        "from skimage.transform import resize as imresize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "# print(\"Keras version \" + tf.keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn_F9m0fpZ9a"
      },
      "outputs": [],
      "source": [
        "# data path\n",
        "TRAIN_PATH = '/content/drive/MyDrive/normal/'\n",
        "VAL_PATH = '/content/drive/MyDrive/'\n",
        "TEST_PATH = '/content/drive/MyDrive/'\n",
        "data_path = TRAIN_PATH\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "# batch size equals to 8 (due to RAM limits)\n",
        "batch_size = 12\n",
        "\n",
        "# define the shape of low resolution image (LR)\n",
        "low_resolution_shape = (32, 32, 1)\n",
        "\n",
        "# define the shape of high resolution image (HR)\n",
        "high_resolution_shape = (224, 224, 1)\n",
        "\n",
        "# optimizer for discriminator, generator\n",
        "common_optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "# use seed for reproducible results\n",
        "SEED = 2020\n",
        "tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzrhpoW9pZ9b"
      },
      "source": [
        "## III. Data\n",
        "\n",
        "Load data, process data, EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKn_m0vepZ9b"
      },
      "outputs": [],
      "source": [
        "def get_train_images(data_path):\n",
        "    # CLASSES = ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n",
        "    CLASSES = [ 'NORMAL']\n",
        "    image_list = []\n",
        "\n",
        "    for class_type in CLASSES:\n",
        "        image_list.extend(glob.glob(data_path + class_type + '/*'))\n",
        "\n",
        "    return image_list\n",
        "get_train_images(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cxww_N4pZ9b"
      },
      "outputs": [],
      "source": [
        "def find_img_dims(image_list):\n",
        "\n",
        "    min_size = []\n",
        "    max_size = []\n",
        "\n",
        "    for i in range(len(image_list)):\n",
        "        im = Image.open(image_list[i])\n",
        "        min_size.append(min(im.size))\n",
        "        max_size.append(max(im.size))\n",
        "\n",
        "    return min(min_size), max(max_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRMLORffpZ9b"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def resize_and_find_img_dims(image_list, target_size=(224, 224)):\n",
        "    min_size = []\n",
        "    max_size = []\n",
        "\n",
        "    for i in range(len(image_list)):\n",
        "        im = Image.open(image_list[i])\n",
        "        im = im.resize(target_size)\n",
        "        min_size.append(min(im.size))\n",
        "        max_size.append(max(im.size))\n",
        "\n",
        "    return min(min_size), max(max_size)\n",
        "\n",
        "# Example usage\n",
        "image_list = get_train_images(data_path)\n",
        "min_size, max_size = resize_and_find_img_dims(image_list)\n",
        "print('The min and max image dims are {} and {} respectively.'.format(min_size, max_size))\n",
        "# print()\n",
        "\n",
        "\n",
        "# image_list = get_train_images(data_path)\n",
        "# min_size, max_size = find_img_dims(image_list)\n",
        "# print('The min and max image dims are {} and {} respectively.'\n",
        "#       .format(min_size, max_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOq9QQLEpZ9c"
      },
      "source": [
        "## IV. Utility functions\n",
        "\n",
        "Quantitative metrics for image quality  \n",
        "Loss functions  \n",
        "Plots  \n",
        "Image processing: sampling and saving images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVg4hdb3pZ9c"
      },
      "source": [
        "### IV A. Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trPud5JOpZ9c"
      },
      "source": [
        "#### 1. PSNR - Peak Signal-to-Noise ratio\n",
        "\n",
        "\n",
        "PSNR is the ratio between maximum possible power of signal and power of corrupting noise (Wikipedia).\n",
        "\n",
        "\n",
        "$${ PSNR = 10  \\log_{10}  \\left( {MAX_I^2 \\over MSE} \\right) }$$\n",
        "\n",
        "$ MAX_I $  -  maximum possible power of a signal of image I  \n",
        "$ MSE $  -  mean squared error pixel by pixel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9cBBQ7rpZ9c"
      },
      "outputs": [],
      "source": [
        "def compute_psnr(original_image, generated_image):\n",
        "\n",
        "    original_image = tf.convert_to_tensor(original_image, dtype=tf.float32)\n",
        "    generated_image = tf.convert_to_tensor(generated_image, dtype=tf.float32)\n",
        "    psnr = tf.image.psnr(original_image, generated_image, max_val=1.0)\n",
        "\n",
        "    return tf.math.reduce_mean(psnr, axis=None, keepdims=False, name=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR4iJ17UpZ9c"
      },
      "outputs": [],
      "source": [
        "def plot_psnr(psnr):\n",
        "\n",
        "    psnr_means = psnr['psnr_quality']\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.plot(psnr_means)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('PSNR')\n",
        "    plt.title('PSNR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUrGuLRhpZ9c"
      },
      "source": [
        "#### 2. SSIM - Structural Similarity Index\n",
        "\n",
        "\n",
        "SSIM measures the perceptual difference between two similar images [(see Wikipedia)](https://en.wikipedia.org/wiki/Structural_similarity).\n",
        "\n",
        "$${ SSIM(x, y) = {(2 \\mu_x \\mu_y + c_1) (2 \\sigma_{xy} + c_2) \\over (\\mu_x^2 + \\mu_y^2 + c_1) ( \\sigma_x^2 + \\sigma_y^2 +c_2)}  }$$\n",
        "\n",
        "\n",
        "$ \\mu_x, \\mu_y$       - average value for image $x, y$    \n",
        "$ \\sigma_x, \\sigma_y$ - standard deviation for image $x, y$     \n",
        "$ \\sigma_{xy}$        - covariance  of $x$ and $y$      \n",
        "$ c_1, c_2 $          - coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4u5JBNVpZ9d"
      },
      "outputs": [],
      "source": [
        "def compute_ssim(original_image, generated_image):\n",
        "\n",
        "    original_image = tf.convert_to_tensor(original_image, dtype=tf.float32)\n",
        "    generated_image = tf.convert_to_tensor(generated_image, dtype=tf.float32)\n",
        "    ssim = tf.image.ssim(original_image, generated_image, max_val=1.0, filter_size=11,\n",
        "                          filter_sigma=1.5, k1=0.01, k2=0.03)\n",
        "\n",
        "    return tf.math.reduce_mean(ssim, axis=None, keepdims=False, name=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U9oFeawpZ9d"
      },
      "outputs": [],
      "source": [
        "def plot_ssim(ssim):\n",
        "\n",
        "    ssim_means = ssim['ssim_quality']\n",
        "\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.plot(ssim_means)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('SSIM')\n",
        "    plt.title('SSIM')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq42BHVVpZ9d"
      },
      "source": [
        "### IV B. Loss Functions\n",
        "\n",
        "The most important contribution of the SRGAN paper was the use of a *perceptual loss* function.\n",
        "\n",
        "\n",
        "**Perceptual Loss**  is a weighted sum of the *content loss* and *adversarial loss*.\n",
        "\n",
        "\n",
        "$${ l^{SR} = l_X^{SR} + 10^{-3}l_{Gen}^{SR}}$$\n",
        "\n",
        "$l^{SR}$ - perceptual loss   \n",
        "$l_X^{SR}$ - content loss   \n",
        "$l_{Gen}^{SR}$ - adversarial loss\n",
        "\n",
        "\n",
        "****************************\n",
        "\n",
        "**1. Content Loss**   \n",
        "The SRGAN replaced the *MSE loss* with a *VGG loss*. Both losses are defined below:\n",
        "\n",
        "         \n",
        "**Pixel-wise MSE loss** is the mean squared error between each pixel in the original HR image and a the corresponding pixel in the generated SR image.\n",
        "\n",
        "\n",
        "**VGG loss** is the euclidean distance between the feature maps of the generated SR image and the original HR  image. The feature maps are the activation layers of the pre-trained  VGG 19 network.\n",
        "\n",
        "$${ l_{{VGG}/{i,j}}^{SR} = {1 \\over {W_{i,j}H_{i,j}}} \\sum\\limits_{x=1}^{W_{i,j}} \\sum\\limits_{y=1}^{H_{i,j}}  ({\\phi}_{i,j}(I^{HR})_{x,y} - {\\phi}_{i,j} (G_{{\\theta}_G} (I^{LR}))_{x,y})^2}$$\n",
        "\n",
        "\n",
        "$ l_{{VGG}/{i,j}}^{SR} $  -  VGG loss    \n",
        "$ {\\phi}_{i,j} $  -   the feature map obtained by the j-th convolution (after activation) before the i-th maxpooling layer within the VGG19 network\n",
        "\n",
        "\n",
        "\n",
        "**2. Adversarial Loss**  \n",
        "This is calculated based on probabilities provided by Discriminator.\n",
        "\n",
        "$${ l_{Gen}^{SR} = \\sum\\limits_{n=1}^{N} - \\log{D_{{\\theta}_D}} (G_{{\\theta}_G} (I^{LR}))}$$\n",
        "\n",
        "$ l_{Gen}^{SR} $  -  generative loss  \n",
        "$ D $  -  discriminator function    \n",
        "$ D_{{\\theta}_D} $  -  discriminator function parametrized with $ {\\theta}_D $   \n",
        "$ {D_{{\\theta}_D}} (G_{{\\theta}_G} (I^{LR})) $   -  probability that the reconstructed image $\n",
        "$ G_{{\\theta}_G} (I^{LR}) $  is a natural HR image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJXQHdIMpZ9d"
      },
      "source": [
        "#### Plot loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgacWn0_pZ9d"
      },
      "outputs": [],
      "source": [
        "def plot_loss(losses):\n",
        "\n",
        "    d_loss = losses['d_history']\n",
        "    g_loss = losses['g_history']\n",
        "\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.plot(d_loss, label=\"Discriminator loss\")\n",
        "    plt.plot(g_loss, label=\"Generator loss\")\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(\"Loss\")\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctsZGNw0pZ9d"
      },
      "source": [
        "### IV C. Sampling, saving images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRPxMGpSpZ9d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "\n",
        "def sample_images(image_list, batch_size, high_resolution_shape, low_resolution_shape):\n",
        "    \"\"\"\n",
        "    Pre-process a batch of training images\n",
        "    \"\"\"\n",
        "    # Randomly sample a batch of images from image_list\n",
        "    images_batch = np.random.choice(image_list, size=batch_size)\n",
        "\n",
        "    lr_images = []\n",
        "    hr_images = []\n",
        "\n",
        "    for img in images_batch:\n",
        "        # Read the image with RGB mode\n",
        "        img1 = imread(img, as_gray=True)\n",
        "        img1 = img1.astype(np.float32)\n",
        "\n",
        "        # Resize the image to high resolution and low resolution\n",
        "        img1_high_resolution = resize(img1, output_shape=(224, 224, 1),anti_aliasing=True)\n",
        "        img1_low_resolution = resize(img1, output_shape=(32, 32, 1),anti_aliasing=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Do a random horizontal flip\n",
        "        if np.random.random() < 0.5:\n",
        "            img1_high_resolution = np.fliplr(img1_high_resolution)\n",
        "            img1_low_resolution = np.fliplr(img1_low_resolution)\n",
        "\n",
        "        hr_images.append(img1_high_resolution)\n",
        "        lr_images.append(img1_low_resolution)\n",
        "\n",
        "    # Convert lists into numpy ndarrays\n",
        "    hr_images = np.array(hr_images)[..., np.newaxis]\n",
        "    lr_images = np.array(lr_images)[..., np.newaxis]\n",
        "    return hr_images, lr_images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWa3FxJozDjD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def save_images(original_image, lr_image, sr_image, path):\n",
        "    \"\"\"\n",
        "    Save LR, HR (original) and generated SR\n",
        "    images in one panel\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(10, 6))\n",
        "\n",
        "    images = [original_image, lr_image, sr_image]\n",
        "    titles = ['HR', 'LR', 'SR - generated']\n",
        "\n",
        "    for idx, img in enumerate(images):\n",
        "        # Ensure image is in [0, 1] range\n",
        "        img = np.clip((img + 1) / 2.0, 0, 1)  # Scale from [-1, 1] to [0, 1]\n",
        "        img = img.squeeze()  # Remove single-dimensional entries from the shape of an array\n",
        "        ax[idx].imshow(img, cmap='gray')\n",
        "        ax[idx].axis(\"off\")\n",
        "        ax[idx].set_title(titles[idx])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAfmdZLNpZ9e"
      },
      "source": [
        "## V. SRGAN-VGG19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaq91DgHpZ9e"
      },
      "source": [
        "The SRGAN has the following code components:\n",
        " 1. Generator network\n",
        " 2.  Discriminator network\n",
        " 3. Feature extractor using the VGG19 network\n",
        " 4. Adversarial framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11Y0TqVspZ9e"
      },
      "source": [
        "### V 1. Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBzieSZOpZ9e"
      },
      "source": [
        "There are 16 residual blocks and 2 upsampling blocks. The generator follows the architecture outlined in [2]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NTr_8Djyqjl"
      },
      "outputs": [],
      "source": [
        "def residual_block(x):\n",
        "    filters = [64, 64]\n",
        "    kernel_size = 3\n",
        "    strides = 1\n",
        "    padding = \"same\"\n",
        "    momentum = 0.8\n",
        "    activation = \"relu\"\n",
        "\n",
        "    res = Conv2D(filters=filters[0], kernel_size=kernel_size, strides=strides, padding=padding)(x)\n",
        "    res = Activation(activation=activation)(res)\n",
        "    res = BatchNormalization(momentum=momentum)(res)\n",
        "\n",
        "    res = Conv2D(filters=filters[1], kernel_size=kernel_size, strides=strides, padding=padding)(res)\n",
        "    res = BatchNormalization(momentum=momentum)(res)\n",
        "\n",
        "    res = Add()([res, x])\n",
        "\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F0NG6TuyvTJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Add, UpSampling2D, Activation, Cropping2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def residual_block(x):\n",
        "    filters = 64\n",
        "    momentum = 0.8\n",
        "\n",
        "    res = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same')(x)\n",
        "    res = BatchNormalization(momentum=momentum)(res)\n",
        "    res = Activation('relu')(res)\n",
        "    res = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same')(res)\n",
        "    res = BatchNormalization(momentum=momentum)(res)\n",
        "\n",
        "    res = Add()([res, x])\n",
        "    return res\n",
        "\n",
        "def build_generator():\n",
        "    residual_blocks = 16\n",
        "    momentum = 0.8\n",
        "    input_shape = (32, 32, 1)\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    gen1 = Conv2D(filters=64, kernel_size=9, strides=1, padding='same', activation='relu')(input_layer)\n",
        "\n",
        "    res = residual_block(gen1)\n",
        "    for i in range(residual_blocks - 1):\n",
        "        res = residual_block(res)\n",
        "\n",
        "    gen2 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(res)\n",
        "    gen2 = BatchNormalization(momentum=momentum)(gen2)\n",
        "\n",
        "    gen1_resize = Conv2D(filters=64, kernel_size=1, strides=1, padding='same')(gen1)\n",
        "    gen3 = Add()([gen2, gen1_resize])\n",
        "\n",
        "    gen4 = UpSampling2D(size=2)(gen3)\n",
        "    gen4 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(gen4)\n",
        "    gen4 = Activation('relu')(gen4)\n",
        "\n",
        "    gen5 = UpSampling2D(size=2)(gen4)\n",
        "    gen5 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(gen5)\n",
        "    gen5 = Activation('relu')(gen5)\n",
        "\n",
        "    gen6 = UpSampling2D(size=2)(gen5)\n",
        "    gen6 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(gen6)\n",
        "    gen6 = Activation('relu')(gen6)\n",
        "\n",
        "    gen7 = Cropping2D(cropping=((16, 16), (16, 16)))(gen6)\n",
        "\n",
        "    gen8 = Conv2D(filters=1, kernel_size=9, strides=1, padding='same')(gen7)\n",
        "    output = Activation('tanh')(gen8)\n",
        "\n",
        "    model = Model(inputs=[input_layer], outputs=[output], name='generator')\n",
        "\n",
        "    return model\n",
        "\n",
        "generator = build_generator()\n",
        "generator.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woeaOXM2pZ9f"
      },
      "source": [
        "### V 2. Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71wEdtSOWb3A"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, LeakyReLU, Dense, Flatten, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def build_discriminator():\n",
        "    # Define hyperparameters\n",
        "    leakyrelu_alpha = 0.2\n",
        "    momentum = 0.8\n",
        "\n",
        "\n",
        "    # Input shape\n",
        "    input_shape = (224, 224, 1)\n",
        "\n",
        "    # Input layer for discriminator\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Convolutional layers with reduced filters\n",
        "    dis1 = Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(input_layer)  # (112, 112, 64)\n",
        "    dis1 = LeakyReLU(alpha=leakyrelu_alpha)(dis1)\n",
        "\n",
        "    dis2 = Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(dis1)  # (56, 56, 64)\n",
        "    dis2 = LeakyReLU(alpha=leakyrelu_alpha)(dis2)\n",
        "    dis2 = BatchNormalization(momentum=momentum)(dis2)\n",
        "\n",
        "    dis3 = Conv2D(filters=128, kernel_size=3, strides=2, padding='same')(dis2)  # (28, 28, 128)\n",
        "    dis3 = LeakyReLU(alpha=leakyrelu_alpha)(dis3)\n",
        "    dis3 = BatchNormalization(momentum=momentum)(dis3)\n",
        "\n",
        "    dis4 = Conv2D(filters=128, kernel_size=3, strides=2, padding='same')(dis3)  # (14, 14, 128)\n",
        "    dis4 = LeakyReLU(alpha=leakyrelu_alpha)(dis4)\n",
        "    dis4 = BatchNormalization(momentum=momentum)(dis4)\n",
        "\n",
        "    dis5 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(dis4)  # (14, 14, 256)\n",
        "    dis5 = LeakyReLU(alpha=leakyrelu_alpha)(dis5)\n",
        "    dis5 = BatchNormalization(momentum=momentum)(dis5)\n",
        "\n",
        "    dis6 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(dis5)  # (14, 14, 256)\n",
        "    dis6 = LeakyReLU(alpha=leakyrelu_alpha)(dis6)\n",
        "    dis6 = BatchNormalization(momentum=momentum)(dis6)\n",
        "\n",
        "    dis7 = Conv2D(filters=512, kernel_size=3, strides=1, padding='same')(dis6)  # (14, 14, 512)\n",
        "    dis7 = LeakyReLU(alpha=leakyrelu_alpha)(dis7)\n",
        "    dis7 = BatchNormalization(momentum=momentum)(dis7)\n",
        "\n",
        "    dis8 = Conv2D(filters=1, kernel_size=3, strides=1, padding='same')(dis7)  # (14, 14, 1)\n",
        "    dis8 = LeakyReLU(alpha=leakyrelu_alpha)(dis8)\n",
        "    dis8 = BatchNormalization(momentum=momentum)(dis8)\n",
        "\n",
        "\n",
        "    output = Dense(units=1, activation='sigmoid')(dis8)\n",
        "    model = Model(inputs=[input_layer], outputs=[output], name='discriminator')\n",
        "\n",
        "    return model\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.trainable = True\n",
        "\n",
        "discriminator.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwjbnDO1pZ9g"
      },
      "source": [
        "### V 3. Mobilenet Extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9imEocCOZW9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.layers import Input, Lambda, Conv2D, UpSampling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def build_teacher_model():\n",
        "    input_shape = (224, 224, 1)\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    rgb_input = Lambda(lambda x: tf.image.grayscale_to_rgb(x))(input_layer)\n",
        "\n",
        "    MobileNet_base = MobileNet(include_top=False, weights='imagenet', input_tensor=rgb_input, input_shape=(224, 224, 3))\n",
        "\n",
        "    output = MobileNet_base.get_layer('conv_pw_11_relu').output  # Shape: (14, 14, 512)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    return model\n",
        "\n",
        "def build_student_model():\n",
        "    input_shape = (224, 224, 1)\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    rgb_input = Lambda(lambda x: tf.image.grayscale_to_rgb(x))(input_layer)\n",
        "\n",
        "    # Use a smaller version of MobileNet\n",
        "    MobileNet_base = MobileNet(include_top=False, weights=None, input_tensor=rgb_input, input_shape=(224, 224, 3), alpha=0.5)\n",
        "\n",
        "    # The output is (7, 7, 512), so we need to upsample\n",
        "    x = MobileNet_base.output\n",
        "    x = UpSampling2D(size=(2, 2))(x)  # Now (14, 14, 512)\n",
        "    output = Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    return model\n",
        "\n",
        "teacher_model = build_teacher_model()\n",
        "teacher_model.trainable = False\n",
        "\n",
        "# Build the student model\n",
        "student_model = build_student_model()\n",
        "\n",
        "# Create a combined model for training\n",
        "input_layer = Input(shape=(224, 224, 1))\n",
        "teacher_preds = teacher_model(input_layer)\n",
        "student_preds = student_model(input_layer)\n",
        "\n",
        "combined_model = Model(inputs=input_layer, outputs=[student_preds, teacher_preds])\n",
        "\n",
        "\n",
        "student_model.compile(optimizer=Adam(), loss='mse')\n",
        "teacher_model.summary(),student_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD9Zz69t9y2z"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "\n",
        "# Define the common optimizer with legacy Adam\n",
        "common_optimizer = Adam(0.0002, 0.5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn0gFU7IpZ9o"
      },
      "outputs": [],
      "source": [
        "def build_adversarial_model(generator, discriminator, teacher_model, student_model):\n",
        "    # Input layer for low-resolution images\n",
        "    input_low_resolution = Input(shape=low_resolution_shape)\n",
        "\n",
        "    # Generate high-resolution images from low-resolution images\n",
        "    generated_high_resolution_images = generator(input_low_resolution)\n",
        "\n",
        "    # Extract features using the teacher model (frozen)\n",
        "    teacher_features = teacher_model(generated_high_resolution_images)\n",
        "\n",
        "    # Extract features using the student model (trainable)\n",
        "    student_features = student_model(generated_high_resolution_images)\n",
        "\n",
        "    # Make discriminator non-trainable for the adversarial model\n",
        "    discriminator.trainable = False\n",
        "\n",
        "    # Discriminator will give us a probability estimation for the generated high-resolution images\n",
        "    probs = discriminator(generated_high_resolution_images)\n",
        "\n",
        "    # Create the adversarial model\n",
        "    adversarial_model = Model(input_low_resolution, [probs, student_features, teacher_features])\n",
        "\n",
        "    return adversarial_model\n",
        "\n",
        "# Ensure the teacher model is not trainable\n",
        "teacher_model.trainable = False\n",
        "\n",
        "# Build the adversarial model\n",
        "adversarial_model = build_adversarial_model(generator, discriminator, teacher_model, student_model)\n",
        "\n",
        "# Compile the adversarial model\n",
        "adversarial_model.compile(\n",
        "    loss=['binary_crossentropy', 'mse', None],  # None for teacher output as it's not trained\n",
        "    loss_weights=[1e-3, 1, 0],  # 0 weight for teacher output\n",
        "    optimizer=common_optimizer\n",
        ")\n",
        "adversarial_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6qxDWXEpZ9p"
      },
      "outputs": [],
      "source": [
        "# adversarial_model = build_adversarial_model(generator, discriminator, fe_model)\n",
        "# adversarial_model.compile(loss=['binary_crossentropy', 'mean_squared_error'],\n",
        "#                            optimizer=common_optimizer,\n",
        "#                            loss_weights=[0.5, 0.5])\n",
        "# adversarial_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryjQb_LXpZ9p"
      },
      "source": [
        "## VI. Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GBRK9egpZ9p"
      },
      "outputs": [],
      "source": [
        "# initialize\n",
        "\n",
        "losses = {\"d_history\":[], \"g_history\":[]}\n",
        "psnr = {'psnr_quality': []}\n",
        "ssim = {'ssim_quality': []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgTMjownQE6U"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array\n",
        "def resize_images(images, target_size):\n",
        "    resized_images = []\n",
        "    for img_array in images:\n",
        "        img = array_to_img(img_array)\n",
        "        img = img.resize(target_size)\n",
        "        img = img_to_array(img)\n",
        "        resized_images.append(img)\n",
        "    return np.array(resized_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x4mUsh3CUV9"
      },
      "outputs": [],
      "source": [
        "\n",
        "discriminator.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mhwip-ACiP2e"
      },
      "outputs": [],
      "source": [
        "hr_images, lr_images = sample_images(image_list,\n",
        "                                         batch_size=batch_size,\n",
        "                                         low_resolution_shape=low_resolution_shape,\n",
        "                                         high_resolution_shape=high_resolution_shape)\n",
        "\n",
        "    # Normalize the images\n",
        "hr_images = hr_images / 127.5 - 1.\n",
        "lr_images = lr_images / 127.5 - 1.\n",
        "if hr_images.shape[-1] == 1 and hr_images.shape[-2] == 1:\n",
        "        hr_images_new = hr_images.reshape(hr_images.shape[:-2] + (1,))\n",
        "if lr_images.shape[-1] == 1 and lr_images.shape[-2] == 1:\n",
        "        lr_images_new = lr_images.reshape(lr_images.shape[:-2] + (1,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ezIL8T1wiJz"
      },
      "outputs": [],
      "source": [
        "len(hr_images)\n",
        "len(image_list)\n",
        "hr_images_new.shape,lr_images_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UZff6adEmNI"
      },
      "outputs": [],
      "source": [
        "\n",
        "for epoch in range(100):\n",
        "    image_list = get_train_images(data_path)\n",
        "\n",
        "    \"\"\"\n",
        "    Train the discriminator network\n",
        "    \"\"\"\n",
        "    hr_images, lr_images = sample_images(image_list,\n",
        "                                         batch_size=batch_size,\n",
        "                                         low_resolution_shape=low_resolution_shape,\n",
        "                                         high_resolution_shape=high_resolution_shape)\n",
        "\n",
        "    # Normalize the images\n",
        "    hr_images = hr_images / 127.5 - 1.\n",
        "    lr_images = lr_images / 127.5 - 1.\n",
        "\n",
        "    # Generate high-resolution images from low-resolution images\n",
        "    generated_high_resolution_images = generator.predict(lr_images)\n",
        "\n",
        "    # Generate a batch of true and fake labels\n",
        "    real_labels = np.ones((batch_size, 14, 14, 1))\n",
        "    fake_labels = np.zeros((batch_size, 14, 14, 1))\n",
        "\n",
        "    d_loss_real = discriminator.train_on_batch(hr_images, real_labels)\n",
        "    d_loss_fake = discriminator.train_on_batch(generated_high_resolution_images, fake_labels)\n",
        "\n",
        "    # Calculate total loss of discriminator as average loss on true and fake labels\n",
        "    d_loss = 0.5 * (np.mean(d_loss_real) + np.mean(d_loss_fake))\n",
        "    losses['d_history'].append(d_loss)\n",
        "\n",
        "    \"\"\"\n",
        "    Train the generator network\n",
        "    \"\"\"\n",
        "    # Extract features using the teacher model\n",
        "    teacher_features = teacher_model.predict(hr_images)\n",
        "\n",
        "    # Train the generator (via the adversarial model)\n",
        "    g_loss = adversarial_model.train_on_batch(lr_images, [real_labels, teacher_features, teacher_features])\n",
        "    losses['g_history'].append(g_loss[1])  # Assuming g_loss[1] is the relevant generator loss\n",
        "\n",
        "    # Calculate PSNR and SSIM\n",
        "    ps = compute_psnr(hr_images_new, generated_high_resolution_images)\n",
        "    ss = compute_ssim(hr_images_new, generated_high_resolution_images)\n",
        "    psnr['psnr_quality'].append(ps)\n",
        "    ssim['ssim_quality'].append(ss)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}  PSNR {ps.numpy()}  SSIM {ss.numpy()}\")\n",
        "\n",
        "    \"\"\"\n",
        "    Save and print image samples\n",
        "    \"\"\"\n",
        "    if epoch % 5 == 0:\n",
        "        for index, img in enumerate(generated_high_resolution_images):\n",
        "            if index < 1:  # Save only one image per 5 epochs for demonstration\n",
        "                save_images(hr_images[index], lr_images[index], img,\n",
        "                            path=f\"/content/drive/MyDrive/mobilenet_distillation/images_{epoch}_{index}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOARRrjDBJtn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
